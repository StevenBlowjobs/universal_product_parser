#!/usr/bin/env python3
"""
–ì–ª–∞–≤–Ω—ã–π CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Universal Product Parser
–ü–æ–ª–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π
"""

import asyncio
import sys
import json
import shutil
from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.adaptive_parser import AdaptiveProductParser
from src.content_processor.text_rewriter import TextRewriter
from src.data_analyzer.data_validator import DataValidator
from src.data_analyzer.price_comparator import PriceComparator
from src.data_analyzer.trend_analyzer import TrendAnalyzer
from src.data_analyzer.alert_system import AlertSystem
from src.exporter.excel_generator import ExcelGenerator
from src.exporter.backup_manager import BackupManager
from src.utils.logger import setup_logger
from src.utils.file_manager import FileManager
from src.utils.error_handler import ParserError, handle_global_error
from .argument_parser import create_parser


class ParserCLI:
    """–ö–æ–º–∞–Ω–¥–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–æ–º —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –º–æ–¥—É–ª–µ–π"""
    
    def __init__(self):
        self.logger = setup_logger("cli")
        self.file_manager = FileManager()
        self.parser = None
        self.target_url = ""
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª–µ–π (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
        self._modules_initialized = False
        self.text_rewriter = None
        self.data_validator = None
        self.price_comparator = None
        self.trend_analyzer = None
        self.alert_system = None
        self.excel_generator = None
        self.backup_manager = None
    
    def _initialize_modules(self, config: Dict[str, Any] = None):
        """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π"""
        if self._modules_initialized:
            return
            
        config = config or {}
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        self.text_rewriter = TextRewriter(config)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª–µ–π –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö
        self.data_validator = DataValidator(config)
        self.price_comparator = PriceComparator(config)
        self.trend_analyzer = TrendAnalyzer(config)
        self.alert_system = AlertSystem(config)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª–µ–π —ç–∫—Å–ø–æ—Ä—Ç–∞
        self.excel_generator = ExcelGenerator(config)
        self.backup_manager = BackupManager()
        
        self._modules_initialized = True
        self.logger.info("‚úÖ –í—Å–µ –º–æ–¥—É–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
    
    async def run(self, args):
        """–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π"""
        try:
            self.logger.info("üöÄ –ó–∞–ø—É—Å–∫ Universal Product Parser —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π")
            self.logger.info("=" * 60)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞
            self.parser = AdaptiveProductParser(args.config)
            await self.parser.initialize()
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ URL
            self.target_url = await self._get_target_url(args)
            if not self.target_url:
                self.logger.error("‚ùå –ù–µ —É–∫–∞–∑–∞–Ω —Ü–µ–ª–µ–≤–æ–π URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
                return 1
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π
            self._initialize_modules(self.parser.config)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤
            filters = self._prepare_filters(args)
            
            # –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞
            self.logger.info(f"üéØ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥: {self.target_url}")
            products = await self.parser.parse_site(self.target_url, filters)
            
            if not products:
                self.logger.warning("‚ö†Ô∏è –¢–æ–≤–∞—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return 0
            
            self.logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(products)} —Ç–æ–≤–∞—Ä–æ–≤")
            
            # –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            processed_data = await self._process_data_pipeline(products, args)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            await self._save_comprehensive_results(processed_data, args.output)
            
            self.logger.info("üéâ –í—Å–µ —ç—Ç–∞–ø—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
            return 0
            
        except ParserError as e:
            handle_global_error(e, {'phase': 'parsing'})
            return 1
        except KeyboardInterrupt:
            self.logger.info("‚èπÔ∏è –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            return 130
        except Exception as e:
            handle_global_error(e, {'phase': 'unknown'})
            return 1
        finally:
            if self.parser:
                await self.parser.close()
    
    async def _process_data_pipeline(self, raw_products: List[Dict], args) -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –≤—Å–µ –º–æ–¥—É–ª–∏
        
        Args:
            raw_products: –°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä–æ–≤
            args: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
            
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö —ç—Ç–∞–ø–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        self.logger.info("üîÑ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
        
        pipeline_results = {
            'timestamp': datetime.now().isoformat(),
            'target_url': self.target_url,
            'original_products_count': len(raw_products)
        }
        
        try:
            # 1. –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            self.logger.info("üîç –≠—Ç–∞–ø 1: –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
            validation_results = self.data_validator.validate_products(raw_products)
            pipeline_results['validation'] = validation_results
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            valid_products = [item['product'] for item in validation_results.get('valid_products', [])]
            self.logger.info(f"‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {len(valid_products)}")
            
            if not valid_products:
                self.logger.warning("‚ö†Ô∏è –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                return pipeline_results
            
            # 2. –ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤
            self.logger.info("üìù –≠—Ç–∞–ø 2: –ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏–π...")
            rewritten_products = []
            
            for product in valid_products:
                try:
                    rewrite_result = self.text_rewriter.rewrite_description(
                        product.get('description', ''),
                        product
                    )
                    
                    if rewrite_result['success']:
                        product['original_description'] = product.get('description', '')
                        product['description'] = rewrite_result['rewritten']
                        product['rewrite_metadata'] = {
                            'success': True,
                            'changes_made': rewrite_result.get('changes_made', {})
                        }
                    else:
                        product['rewrite_metadata'] = {
                            'success': False,
                            'error': rewrite_result.get('error', 'Unknown error')
                        }
                    
                    rewritten_products.append(product)
                    
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–∞ {product.get('name')}: {e}")
                    product['rewrite_metadata'] = {'success': False, 'error': str(e)}
                    rewritten_products.append(product)
            
            pipeline_results['rewriting'] = {
                'total_processed': len(rewritten_products),
                'successful_rewrites': sum(1 for p in rewritten_products 
                                         if p.get('rewrite_metadata', {}).get('success', False))
            }
            
            # 3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–Ω
            self.logger.info("üìä –≠—Ç–∞–ø 3: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–Ω...")
            price_comparison = self.price_comparator.compare_prices(
                current_data=rewritten_products,
                previous_file_path="data/output/previous_products.json"
            )
            pipeline_results['price_comparison'] = price_comparison
            
            # 4. –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ)
            self.logger.info("üìà –≠—Ç–∞–ø 4: –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤...")
            trend_analysis = await self._analyze_trends(rewritten_products)
            pipeline_results['trend_analysis'] = trend_analysis
            
            # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π
            self.logger.info("üö® –≠—Ç–∞–ø 5: –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π...")
            alerts = self.alert_system.check_for_alerts(
                price_comparison=price_comparison,
                trend_analysis=trend_analysis,
                validation_results=validation_results
            )
            pipeline_results['alerts'] = alerts
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –≤ –∫–æ–Ω—Å–æ–ª—å
            self.alert_system.send_alerts(alerts, method='console')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            self._save_current_data(rewritten_products)
            
            pipeline_results['processed_products'] = rewritten_products
            pipeline_results['success'] = True
            
            self.logger.info("‚úÖ –ö–æ–Ω–≤–µ–π–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω")
            return pipeline_results
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∫–æ–Ω–≤–µ–π–µ—Ä–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            pipeline_results['success'] = False
            pipeline_results['error'] = str(e)
            return pipeline_results
    
    async def _analyze_trends(self, products: List[Dict]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            history_file = Path("data/output/price_history.json")
            if history_file.exists():
                with open(history_file, 'r', encoding='utf-8') as f:
                    price_history = json.load(f)
                
                # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
                return self.trend_analyzer.analyze_price_trends(price_history)
            else:
                return {
                    'success': False,
                    'reason': '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤'
                }
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _save_current_data(self, products: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±—É–¥—É—â–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        try:
            output_dir = Path("data/output")
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ü–µ–Ω
            with open(output_dir / "previous_products.json", 'w', encoding='utf-8') as f:
                json.dump(products, f, ensure_ascii=False, indent=2)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —Ü–µ–Ω
            self._update_price_history(products)
            
            self.logger.info("üíæ –¢–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–ª—è –±—É–¥—É—â–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
    
    def _update_price_history(self, products: List[Dict]):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —Ü–µ–Ω"""
        try:
            history_file = Path("data/output/price_history.json")
            history = {}
            
            if history_file.exists():
                with open(history_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
            
            timestamp = datetime.now().isoformat()
            site_key = self._get_site_key(self.target_url)
            
            if site_key not in history:
                history[site_key] = {}
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö —Ü–µ–Ω –≤ –∏—Å—Ç–æ—Ä–∏—é
            for product in products:
                product_key = self.price_comparator._get_product_key(product)
                if product_key not in history[site_key]:
                    history[site_key][product_key] = []
                
                history[site_key][product_key].append({
                    'timestamp': timestamp,
                    'price': product.get('price'),
                    'name': product.get('name', '')
                })
                
                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 –∑–∞–ø–∏—Å–µ–π –Ω–∞ —Ç–æ–≤–∞—Ä)
                if len(history[site_key][product_key]) > 100:
                    history[site_key][product_key] = history[site_key][product_key][-100:]
            
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(history, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —Ü–µ–Ω: {e}")
    
    async def _save_comprehensive_results(self, processed_data: Dict[str, Any], output_path: Optional[str]):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤—Å–µ –º–æ–¥—É–ª–∏ —ç–∫—Å–ø–æ—Ä—Ç–∞"""
        try:
            self.logger.info("üíæ –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
            
            products = processed_data.get('processed_products', [])
            
            if not products:
                self.logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
                return
            
            # 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Excel –æ—Ç—á–µ—Ç–∞
            self.logger.info("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ Excel –æ—Ç—á–µ—Ç–∞...")
            excel_file = self.excel_generator.generate_comprehensive_report(
                products=products,
                price_comparison=processed_data.get('price_comparison'),
                trend_analysis=processed_data.get('trend_analysis'),
                validation_results=processed_data.get('validation'),
                alerts=processed_data.get('alerts'),
                site_url=self.target_url
            )
            
            # 2. –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏
            self.logger.info("üíæ –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏...")
            backup_path = self.backup_manager.create_backup(
                data_files=['output/previous_products.json', 'output/price_history.json', 'output/excel_exports/*'],
                description=f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –±—ç–∫–∞–ø –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ {self.target_url}"
            )
            
            # 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            debug_file = Path("data/output/debug_processing.json")
            with open(debug_file, 'w', encoding='utf-8') as f:
                json.dump(processed_data, f, ensure_ascii=False, indent=2)
            
            # 4. –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self._print_comprehensive_summary(processed_data, excel_file, backup_path)
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
            # Fallback: –ø—Ä–æ—Å—Ç–æ–π —ç–∫—Å–ø–æ—Ä—Ç
            await self._save_fallback(products, output_path)
    
    def _print_comprehensive_summary(self, processed_data: Dict, excel_file: str, backup_path: str):
        """–í—ã–≤–æ–¥ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π —Å–≤–æ–¥–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        products = processed_data.get('processed_products', [])
        validation = processed_data.get('validation', {})
        price_comparison = processed_data.get('price_comparison', {})
        alerts = processed_data.get('alerts', {})
        
        print("\n" + "="*70)
        print("üéâ –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –û–¢–ß–ï–¢ –ó–ê–í–ï–†–®–ï–ù!")
        print("="*70)
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}")
        print(f"‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {validation.get('quality_metrics', {}).get('valid_products_count', 0)}")
        print(f"üíæ Excel –æ—Ç—á–µ—Ç: {excel_file}")
        print(f"üíæ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è: {backup_path}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏—è
        rewriting = processed_data.get('rewriting', {})
        print(f"üìù –ü–µ—Ä–µ–ø–∏—Å–∞–Ω–æ –æ–ø–∏—Å–∞–Ω–∏–π: {rewriting.get('successful_rewrites', 0)}/{rewriting.get('total_processed', 0)}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ü–µ–Ω
        if price_comparison.get('success'):
            price_changes = price_comparison.get('price_changes', {})
            new_products = len(price_comparison.get('new_products', []))
            print(f"üí∞ –ò–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω: +{price_changes.get('increased', 0)}/-{price_changes.get('decreased', 0)}")
            print(f"üÜï –ù–æ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {new_products}")
        
        # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
        alert_summary = alerts.get('summary', {})
        if alert_summary.get('total_alerts', 0) > 0:
            print(f"üö® –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–π: {alert_summary['total_alerts']} "
                  f"(–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö: {alert_summary.get('critical_alerts', 0)})")
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
        quality_metrics = validation.get('quality_metrics', {})
        print(f"üìà –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: {quality_metrics.get('quality_score', 0):.1%}")
        
        print("="*70)
    
    async def _get_target_url(self, args) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ URL –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ —Ñ–∞–π–ª–∞"""
        if args.url:
            return args.url
        elif args.url_file:
            try:
                with open(args.url_file, 'r', encoding='utf-8') as f:
                    urls = [line.strip() for line in f if line.strip()]
                return urls[0] if urls else None
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ URL: {e}")
                return None
        else:
            # –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            default_url_file = Path("data/input/target_urls.txt")
            if default_url_file.exists():
                try:
                    with open(default_url_file, 'r', encoding='utf-8') as f:
                        urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                    return urls[0] if urls else None
                except Exception as e:
                    self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ target_urls.txt: {e}")
        
        return None
    
    def _prepare_filters(self, args) -> dict:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
        filters = {}
        
        # –§–∏–ª—å—Ç—Ä –ø–æ —Ü–µ–Ω–µ
        if args.min_price or args.max_price:
            filters['price_range'] = {}
            if args.min_price:
                filters['price_range']['min'] = args.min_price
            if args.max_price:
                filters['price_range']['max'] = args.max_price
        
        # –§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        if args.categories:
            filters['categories'] = [cat.strip() for cat in args.categories.split(',')]
        
        # –§–∏–ª—å—Ç—Ä –ø–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º
        if args.filters:
            try:
                # –û–∂–∏–¥–∞–µ–º —Ñ–æ—Ä–º–∞—Ç: "–≤–µ—Å:0.1-10,–¥–ª–∏–Ω–∞:100-500"
                char_filters = {}
                for filter_pair in args.filters.split(','):
                    if ':' in filter_pair:
                        key, value = filter_pair.split(':', 1)
                        char_filters[key.strip()] = value.strip()
                if char_filters:
                    filters['characteristics'] = char_filters
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤: {e}")
        
        return filters
    
    def _get_site_key(self, url: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–∞ –¥–ª—è —Å–∞–π—Ç–∞"""
        return url.replace('https://', '').replace('http://', '').split('/')[0]
    
    async def _save_fallback(self, products: List[dict], output_path: Optional[str]):
        """–†–µ–∑–µ—Ä–≤–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON"""
        try:
            output_file = output_path or "data/output/parsed_products.json"
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(products, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"üíæ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_file}")
        except Exception as e:
            self.logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
    
    async def test_connection(self, url: str):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å —Å–∞–π—Ç–æ–º"""
        try:
            self.logger.info(f"üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å {url}")
            
            from src.utils.network_utils import NetworkUtils
            network_utils = NetworkUtils()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
            if not await network_utils.check_internet_connection():
                self.logger.error("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
                return False
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–∞–π—Ç–∞
            site_check = await network_utils.check_site_availability(url)
            
            if site_check['available']:
                self.logger.info(f"‚úÖ –°–∞–π—Ç –¥–æ—Å—Ç—É–ø–µ–Ω, –æ—Ç–≤–µ—Ç: {site_check['status_code']}")
                self.logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {site_check['response_time']} —Å–µ–∫")
                return True
            else:
                self.logger.error(f"‚ùå –°–∞–π—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {site_check.get('error', 'Unknown error')}")
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return False
    
    def show_stats(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—à–∏–±–æ–∫"""
        from src.utils.error_handler import get_global_error_stats
        stats = get_global_error_stats()
        
        print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–®–ò–ë–û–ö:")
        print("=" * 30)
        print(f"–í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {stats['total_errors']}")
        print(f"–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ: {stats['recovered_errors']}")
        print(f"–ü–æ —Ç–∏–ø–∞–º: {stats['by_type']}")


async def main():#!/usr/bin/env python3
"""
–ì–ª–∞–≤–Ω—ã–π CLI –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å Universal Product Parser
–ü–æ–ª–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∞—Ä—Ç–∏–∫—É–ª–æ–≤ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
"""

import asyncio
import sys
import json
import shutil
from pathlib import Path
from typing import List, Optional, Dict, Any
from datetime import datetime

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from src.core.adaptive_parser import AdaptiveProductParser
from src.content_processor.text_rewriter import TextRewriter
from src.data_analyzer.data_validator import DataValidator
from src.data_analyzer.price_comparator import PriceComparator
from src.data_analyzer.trend_analyzer import TrendAnalyzer
from src.data_analyzer.alert_system import AlertSystem
from src.exporter.excel_generator import ExcelGenerator
from src.exporter.backup_manager import BackupManager
from src.utils.logger import setup_logger
from src.utils.file_manager import FileManager
from src.utils.error_handler import ParserError, handle_global_error
from .argument_parser import create_parser


class ParserCLI:
    """–ö–æ–º–∞–Ω–¥–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–æ–º —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –º–æ–¥—É–ª–µ–π"""
    
    def __init__(self):
        self.logger = setup_logger("cli")
        self.file_manager = FileManager()
        self.parser = None
        self.target_url = ""
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª–µ–π (–ª–µ–Ω–∏–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞)
        self._modules_initialized = False
        self.text_rewriter = None
        self.data_validator = None
        self.price_comparator = None
        self.trend_analyzer = None
        self.alert_system = None
        self.excel_generator = None
        self.backup_manager = None
    
    def _initialize_modules(self, config: Dict[str, Any] = None):
        """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π"""
        if self._modules_initialized:
            return
            
        config = config or {}
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        self.text_rewriter = TextRewriter(config)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª–µ–π –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö
        self.data_validator = DataValidator(config)
        self.price_comparator = PriceComparator(config)
        self.trend_analyzer = TrendAnalyzer(config)
        self.alert_system = AlertSystem(config)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥—É–ª–µ–π —ç–∫—Å–ø–æ—Ä—Ç–∞
        self.excel_generator = ExcelGenerator(config)
        self.backup_manager = BackupManager()
        
        self._modules_initialized = True
        self.logger.info("‚úÖ –í—Å–µ –º–æ–¥—É–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
    
    async def run(self, args):
        """–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π"""
        try:
            self.logger.info("üöÄ –ó–∞–ø—É—Å–∫ Universal Product Parser —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π")
            self.logger.info("=" * 60)
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞
            self.parser = AdaptiveProductParser(args.config)
            await self.parser.initialize()
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ URL
            self.target_url = await self._get_target_url(args)
            if not self.target_url:
                self.logger.error("‚ùå –ù–µ —É–∫–∞–∑–∞–Ω —Ü–µ–ª–µ–≤–æ–π URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞")
                return 1
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π
            self._initialize_modules(self.parser.config)
            
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤
            filters = self._prepare_filters(args)
            
            # –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞
            self.logger.info(f"üéØ –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥: {self.target_url}")
            products = await self.parser.parse_site(self.target_url, filters)
            
            if not products:
                self.logger.warning("‚ö†Ô∏è –¢–æ–≤–∞—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
                return 0
            
            self.logger.info(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {len(products)} —Ç–æ–≤–∞—Ä–æ–≤")
            
            # –ü–æ–ª–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            processed_data = await self._process_data_pipeline(products, args)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            await self._save_comprehensive_results(processed_data, args.output)
            
            self.logger.info("üéâ –í—Å–µ —ç—Ç–∞–ø—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
            return 0
            
        except ParserError as e:
            handle_global_error(e, {'phase': 'parsing'})
            return 1
        except KeyboardInterrupt:
            self.logger.info("‚èπÔ∏è –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            return 130
        except Exception as e:
            handle_global_error(e, {'phase': 'unknown'})
            return 1
        finally:
            if self.parser:
                await self.parser.close()
    
    async def _process_data_pipeline(self, raw_products: List[Dict], args) -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω—ã–π –∫–æ–Ω–≤–µ–π–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –≤—Å–µ –º–æ–¥—É–ª—ã
        
        Args:
            raw_products: –°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä–æ–≤ (—É–∂–µ —Å –∞—Ä—Ç–∏–∫—É–ª–∞–º–∏ –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏)
            args: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
            
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—Å–µ—Ö —ç—Ç–∞–ø–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        """
        self.logger.info("üîÑ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∫–æ–Ω–≤–µ–π–µ—Ä–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö")
        
        pipeline_results = {
            'timestamp': datetime.now().isoformat(),
            'target_url': self.target_url,
            'original_products_count': len(raw_products)
        }
        
        try:
            # –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ù–û–í–´–ú –§–£–ù–ö–¶–ò–Ø–ú
            articles_generated = sum(1 for p in raw_products if p.get('article'))
            total_images = sum(
                len(p.get('processed_images', {}).get('processed_images', [])) 
                for p in raw_products
            )
            approved_images = sum(
                p.get('processed_images', {}).get('moderation_results', {}).get('approved_count', 0)
                for p in raw_products
            )
            
            self.logger.info(f"üìù –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∞—Ä—Ç–∏–∫—É–ª–æ–≤: {articles_generated}/{len(raw_products)}")
            self.logger.info(f"üñºÔ∏è –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {approved_images}/{total_images} –æ–¥–æ–±—Ä–µ–Ω–æ")
            
            # 1. –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
            self.logger.info("üîç –≠—Ç–∞–ø 1: –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
            validation_results = self.data_validator.validate_products(raw_products)
            pipeline_results['validation'] = validation_results
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –≤–∞–ª–∏–¥–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            valid_products = [item['product'] for item in validation_results.get('valid_products', [])]
            self.logger.info(f"‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {len(valid_products)}")
            
            if not valid_products:
                self.logger.warning("‚ö†Ô∏è –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏")
                return pipeline_results
            
            # 2. –ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤
            self.logger.info("üìù –≠—Ç–∞–ø 2: –ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏–π...")
            rewritten_products = []
            
            for product in valid_products:
                try:
                    rewrite_result = self.text_rewriter.rewrite_description(
                        product.get('description', ''),
                        product
                    )
                    
                    if rewrite_result['success']:
                        product['original_description'] = product.get('description', '')
                        product['description'] = rewrite_result['rewritten']
                        product['rewrite_metadata'] = {
                            'success': True,
                            'changes_made': rewrite_result.get('changes_made', {})
                        }
                    else:
                        product['rewrite_metadata'] = {
                            'success': False,
                            'error': rewrite_result.get('error', 'Unknown error')
                        }
                    
                    rewritten_products.append(product)
                    
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–∞ {product.get('name')}: {e}")
                    product['rewrite_metadata'] = {'success': False, 'error': str(e)}
                    rewritten_products.append(product)
            
            pipeline_results['rewriting'] = {
                'total_processed': len(rewritten_products),
                'successful_rewrites': sum(1 for p in rewritten_products 
                                         if p.get('rewrite_metadata', {}).get('success', False)),
                'articles_generated': articles_generated,
                'images_processed': total_images,
                'images_approved': approved_images
            }
            
            # 3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–Ω
            self.logger.info("üìä –≠—Ç–∞–ø 3: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ü–µ–Ω...")
            price_comparison = self.price_comparator.compare_prices(
                current_data=rewritten_products,
                previous_file_path="data/output/previous_products.json"
            )
            pipeline_results['price_comparison'] = price_comparison
            
            # 4. –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ)
            self.logger.info("üìà –≠—Ç–∞–ø 4: –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤...")
            trend_analysis = await self._analyze_trends(rewritten_products)
            pipeline_results['trend_analysis'] = trend_analysis
            
            # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π
            self.logger.info("üö® –≠—Ç–∞–ø 5: –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π...")
            alerts = self.alert_system.check_for_alerts(
                price_comparison=price_comparison,
                trend_analysis=trend_analysis,
                validation_results=validation_results
            )
            pipeline_results['alerts'] = alerts
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –≤ –∫–æ–Ω—Å–æ–ª—å
            self.alert_system.send_alerts(alerts, method='console')
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            self._save_current_data(rewritten_products)
            
            pipeline_results['processed_products'] = rewritten_products
            pipeline_results['success'] = True
            
            self.logger.info("‚úÖ –ö–æ–Ω–≤–µ–π–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω")
            return pipeline_results
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ –∫–æ–Ω–≤–µ–π–µ—Ä–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            pipeline_results['success'] = False
            pipeline_results['error'] = str(e)
            return pipeline_results
    
    async def _analyze_trends(self, products: List[Dict]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            history_file = Path("data/output/price_history.json")
            if history_file.exists():
                with open(history_file, 'r', encoding='utf-8') as f:
                    price_history = json.load(f)
                
                # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
                return self.trend_analyzer.analyze_price_trends(price_history)
            else:
                return {
                    'success': False,
                    'reason': '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤'
                }
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def _save_current_data(self, products: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±—É–¥—É—â–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        try:
            output_dir = Path("data/output")
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ü–µ–Ω
            with open(output_dir / "previous_products.json", 'w', encoding='utf-8') as f:
                json.dump(products, f, ensure_ascii=False, indent=2)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —Ü–µ–Ω
            self._update_price_history(products)
            
            self.logger.info("üíæ –¢–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–ª—è –±—É–¥—É—â–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {e}")
    
    def _update_price_history(self, products: List[Dict]):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —Ü–µ–Ω"""
        try:
            history_file = Path("data/output/price_history.json")
            history = {}
            
            if history_file.exists():
                with open(history_file, 'r', encoding='utf-8') as f:
                    history = json.load(f)
            
            timestamp = datetime.now().isoformat()
            site_key = self._get_site_key(self.target_url)
            
            if site_key not in history:
                history[site_key] = {}
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö —Ü–µ–Ω –≤ –∏—Å—Ç–æ—Ä–∏—é
            for product in products:
                product_key = self.price_comparator._get_product_key(product)
                if product_key not in history[site_key]:
                    history[site_key][product_key] = []
                
                history[site_key][product_key].append({
                    'timestamp': timestamp,
                    'price': product.get('price'),
                    'name': product.get('name', ''),
                    'article': product.get('article', '')  # –î–û–ë–ê–í–õ–ï–ù–û: –∞—Ä—Ç–∏–∫—É–ª –≤ –∏—Å—Ç–æ—Ä–∏—é
                })
                
                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 100 –∑–∞–ø–∏—Å–µ–π –Ω–∞ —Ç–æ–≤–∞—Ä)
                if len(history[site_key][product_key]) > 100:
                    history[site_key][product_key] = history[site_key][product_key][-100:]
            
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(history, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —Ü–µ–Ω: {e}")
    
    async def _save_comprehensive_results(self, processed_data: Dict[str, Any], output_path: Optional[str]):
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —á–µ—Ä–µ–∑ –≤—Å–µ –º–æ–¥—É–ª–∏ —ç–∫—Å–ø–æ—Ä—Ç–∞"""
        try:
            self.logger.info("üíæ –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
            
            products = processed_data.get('processed_products', [])
            
            if not products:
                self.logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
                return
            
            # 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Excel –æ—Ç—á–µ—Ç–∞ —Å –Ω–æ–≤–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
            self.logger.info("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ Excel –æ—Ç—á–µ—Ç–∞...")
            excel_file = self.excel_generator.generate_comprehensive_report(
                products=products,
                price_comparison=processed_data.get('price_comparison'),
                trend_analysis=processed_data.get('trend_analysis'),
                validation_results=processed_data.get('validation'),
                alerts=processed_data.get('alerts'),
                site_url=self.target_url
            )
            
            # 2. –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏
            self.logger.info("üíæ –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏...")
            backup_path = self.backup_manager.create_backup(
                data_files=['output/previous_products.json', 'output/price_history.json', 'output/excel_exports/*'],
                description=f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –±—ç–∫–∞–ø –ø–æ—Å–ª–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ {self.target_url}"
            )
            
            # 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            debug_file = Path("data/output/debug_processing.json")
            with open(debug_file, 'w', encoding='utf-8') as f:
                json.dump(processed_data, f, ensure_ascii=False, indent=2)
            
            # 4. –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            self._print_comprehensive_summary(processed_data, excel_file, backup_path)
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
            # Fallback: –ø—Ä–æ—Å—Ç–æ–π —ç–∫—Å–ø–æ—Ä—Ç
            await self._save_fallback(products, output_path)
    
    def _print_comprehensive_summary(self, processed_data: Dict, excel_file: str, backup_path: str):
        """–í—ã–≤–æ–¥ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π —Å–≤–æ–¥–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        products = processed_data.get('processed_products', [])
        validation = processed_data.get('validation', {})
        price_comparison = processed_data.get('price_comparison', {})
        alerts = processed_data.get('alerts', {})
        rewriting = processed_data.get('rewriting', {})
        
        print("\n" + "="*70)
        print("üéâ –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –û–¢–ß–ï–¢ –ó–ê–í–ï–†–®–ï–ù!")
        print("="*70)
        
        # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(products)}")
        print(f"‚úÖ –í–∞–ª–∏–¥–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {validation.get('quality_metrics', {}).get('valid_products_count', 0)}")
        print(f"üíæ Excel –æ—Ç—á–µ—Ç: {excel_file}")
        print(f"üíæ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è: {backup_path}")
        
        # –ù–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê: –ê—Ä—Ç–∏–∫—É–ª—ã –∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        print(f"üìù –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –∞—Ä—Ç–∏–∫—É–ª–æ–≤: {rewriting.get('articles_generated', 0)}")
        print(f"üñºÔ∏è –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {rewriting.get('images_approved', 0)}/{rewriting.get('images_processed', 0)} –æ–¥–æ–±—Ä–µ–Ω–æ")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏—è
        print(f"üìù –ü–µ—Ä–µ–ø–∏—Å–∞–Ω–æ –æ–ø–∏—Å–∞–Ω–∏–π: {rewriting.get('successful_rewrites', 0)}/{rewriting.get('total_processed', 0)}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ü–µ–Ω
        if price_comparison.get('success'):
            price_changes = price_comparison.get('price_changes', {})
            new_products = len(price_comparison.get('new_products', []))
            print(f"üí∞ –ò–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω: +{price_changes.get('increased', 0)}/-{price_changes.get('decreased', 0)}")
            print(f"üÜï –ù–æ–≤—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {new_products}")
        
        # –£–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
        alert_summary = alerts.get('summary', {})
        if alert_summary.get('total_alerts', 0) > 0:
            print(f"üö® –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–π: {alert_summary['total_alerts']} "
                  f"(–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö: {alert_summary.get('critical_alerts', 0)})")
        
        # –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
        quality_metrics = validation.get('quality_metrics', {})
        print(f"üìà –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: {quality_metrics.get('quality_score', 0):.1%}")
        
        print("="*70)
    
    async def _get_target_url(self, args) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–≥–æ URL –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∏–ª–∏ —Ñ–∞–π–ª–∞"""
        if args.url:
            return args.url
        elif args.url_file:
            try:
                with open(args.url_file, 'r', encoding='utf-8') as f:
                    urls = [line.strip() for line in f if line.strip()]
                return urls[0] if urls else None
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ URL: {e}")
                return None
        else:
            # –ü–æ–ø—ã—Ç–∫–∞ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            default_url_file = Path("data/input/target_urls.txt")
            if default_url_file.exists():
                try:
                    with open(default_url_file, 'r', encoding='utf-8') as f:
                        urls = [line.strip() for line in f if line.strip() and not line.startswith('#')]
                    return urls[0] if urls else None
                except Exception as e:
                    self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ target_urls.txt: {e}")
        
        return None
    
    def _prepare_filters(self, args) -> dict:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
        filters = {}
        
        # –§–∏–ª—å—Ç—Ä –ø–æ —Ü–µ–Ω–µ
        if args.min_price or args.max_price:
            filters['price_range'] = {}
            if args.min_price:
                filters['price_range']['min'] = args.min_price
            if args.max_price:
                filters['price_range']['max'] = args.max_price
        
        # –§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        if args.categories:
            filters['categories'] = [cat.strip() for cat in args.categories.split(',')]
        
        # –§–∏–ª—å—Ç—Ä –ø–æ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º
        if args.filters:
            try:
                # –û–∂–∏–¥–∞–µ–º —Ñ–æ—Ä–º–∞—Ç: "–≤–µ—Å:0.1-10,–¥–ª–∏–Ω–∞:100-500"
                char_filters = {}
                for filter_pair in args.filters.split(','):
                    if ':' in filter_pair:
                        key, value = filter_pair.split(':', 1)
                        char_filters[key.strip()] = value.strip()
                if char_filters:
                    filters['characteristics'] = char_filters
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–∏–ª—å—Ç—Ä–æ–≤: {e}")
        
        return filters
    
    def _get_site_key(self, url: str) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–∞ –¥–ª—è —Å–∞–π—Ç–∞"""
        return url.replace('https://', '').replace('http://', '').split('/')[0]
    
    async def _save_fallback(self, products: List[dict], output_path: Optional[str]):
        """–†–µ–∑–µ—Ä–≤–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON"""
        try:
            output_file = output_path or "data/output/parsed_products.json"
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(products, f, ensure_ascii=False, indent=2)
            
            self.logger.info(f"üíæ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {output_file}")
        except Exception as e:
            self.logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
    
    async def test_connection(self, url: str):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å —Å–∞–π—Ç–æ–º"""
        try:
            self.logger.info(f"üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å {url}")
            
            from src.utils.network_utils import NetworkUtils
            network_utils = NetworkUtils()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞
            if not await network_utils.check_internet_connection():
                self.logger.error("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ")
                return False
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–∞–π—Ç–∞
            site_check = await network_utils.check_site_availability(url)
            
            if site_check['available']:
                self.logger.info(f"‚úÖ –°–∞–π—Ç –¥–æ—Å—Ç—É–ø–µ–Ω, –æ—Ç–≤–µ—Ç: {site_check['status_code']}")
                self.logger.info(f"‚è±Ô∏è –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞: {site_check['response_time']} —Å–µ–∫")
                return True
            else:
                self.logger.error(f"‚ùå –°–∞–π—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {site_check.get('error', 'Unknown error')}")
                return False
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            return False
    
    def show_stats(self):
        """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ—à–∏–±–æ–∫"""
        from src.utils.error_handler import get_global_error_stats
        stats = get_global_error_stats()
        
        print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–®–ò–ë–û–ö:")
        print("=" * 30)
        print(f"–í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {stats['total_errors']}")
        print(f"–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ: {stats['recovered_errors']}")
        print(f"–ü–æ —Ç–∏–ø–∞–º: {stats['by_type']}")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI"""
    parser = create_parser()
    args = parser.parse_args()
    
    cli = ParserCLI()
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –∫–æ–º–∞–Ω–¥
    if args.command == 'test':
        if not args.url:
            print("‚ùå –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —É–∫–∞–∂–∏—Ç–µ URL —Å –ø–æ–º–æ—â—å—é --url")
            return 1
        success = await cli.test_connection(args.url)
        return 0 if success else 1
    
    elif args.command == 'stats':
        cli.show_stats()
        return 0
    
    elif args.command == 'parse':
        return await cli.run(args)
    
    else:
        # –ï—Å–ª–∏ –∫–æ–º–∞–Ω–¥–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º help
        parser.print_help()
        return 0


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –≥–ª–∞–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è CLI"""
    parser = create_parser()
    args = parser.parse_args()
    
    cli = ParserCLI()
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –∫–æ–º–∞–Ω–¥
    if args.command == 'test':
        if not args.url:
            print("‚ùå –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —É–∫–∞–∂–∏—Ç–µ URL —Å –ø–æ–º–æ—â—å—é --url")
            return 1
        success = await cli.test_connection(args.url)
        return 0 if success else 1
    
    elif args.command == 'stats':
        cli.show_stats()
        return 0
    
    elif args.command == 'parse':
        return await cli.run(args)
    
    else:
        # –ï—Å–ª–∏ –∫–æ–º–∞–Ω–¥–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º help
        parser.print_help()
        return 0


if __name__ == "__main__":
    # –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –≥–ª–∞–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
