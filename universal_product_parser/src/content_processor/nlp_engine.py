#!/usr/bin/env python3
"""
NLP –¥–≤–∏–∂–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞
"""

import spacy
import pymorphy3
import nltk
from typing import Dict, List, Optional, Any, Tuple
from ..utils.logger import setup_logger
from ..utils.error_handler import retry_on_failure


class NlpEngine:
    """NLP –¥–≤–∏–∂–æ–∫ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
    
    def __init__(self):
        self.logger = setup_logger("nlp_engine")
        self._load_models()
    
    def _load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ NLP –º–æ–¥–µ–ª–µ–π –∏ —Ä–µ—Å—É—Ä—Å–æ–≤"""
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ spaCy –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
            self.logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ spaCy –º–æ–¥–µ–ª–∏...")
            self.nlp = spacy.load("ru_core_news_lg")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è pymorphy3 –¥–ª—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            self.logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ pymorphy3...")
            self.morph_analyzer = pymorphy3.MorphAnalyzer()
            
            # –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤ NLTK
            self.logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ NLTK —Ä–µ—Å—É—Ä—Å–æ–≤...")
            try:
                nltk.data.find('tokenizers/punkt')
            except LookupError:
                nltk.download('punkt')
            
            self.logger.info("‚úÖ NLP –º–æ–¥–µ–ª–∏ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ NLP –º–æ–¥–µ–ª–µ–π: {e}")
            raise
    
    def analyze_text(self, text: str) -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            Dict: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        """
        if not text:
            return {}
        
        doc = self.nlp(text)
        
        sentences = []
        for sent in doc.sents:
            sentence_analysis = {
                'text': sent.text,
                'tokens': [],
                'entities': [],
                'pos_tags': []
            }
            
            for token in sent:
                token_info = {
                    'text': token.text,
                    'lemma': token.lemma_,
                    'pos': token.pos_,
                    'tag': token.tag_,
                    'dep': token.dep_,
                    'is_alpha': token.is_alpha,
                    'is_stop': token.is_stop
                }
                sentence_analysis['tokens'].append(token_info)
                sentence_analysis['pos_tags'].append(token.pos_)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π
            for ent in sent.ents:
                sentence_analysis['entities'].append({
                    'text': ent.text,
                    'label': ent.label_,
                    'start': ent.start_char - sent.start_char,
                    'end': ent.end_char - sent.start_char
                })
            
            sentences.append(sentence_analysis)
        
        return {
            'original_text': text,
            'sentences': sentences,
            'word_count': len([token for token in doc if token.is_alpha]),
            'sentence_count': len(list(doc.sents)),
            'language': 'ru'
        }
    
    def extract_brands_and_models(self, text: str) -> Dict[str, List[str]]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±—Ä–µ–Ω–¥–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            Dict: –°–ø–∏—Å–∫–∏ –±—Ä–µ–Ω–¥–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π
        """
        result = {
            'brands': [],
            'models': []
        }
        
        if not text:
            return result
        
        doc = self.nlp(text)
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤ –∏ –º–æ–¥–µ–ª–µ–π
        brand_indicators = ['–±—Ä–µ–Ω–¥', '–º–∞—Ä–∫–∞', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å', 'brand', 'make']
        model_indicators = ['–º–æ–¥–µ–ª—å', 'version', '—Ç–∏–ø', 'model']
        
        for sent in doc.sents:
            for token in sent:
                # –ü–æ–∏—Å–∫ –±—Ä–µ–Ω–¥–æ–≤ (–æ–±—ã—á–Ω–æ –∏–º–µ–Ω–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ)
                if token.ent_type_ == 'ORG' or token.is_title:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –±—Ä–µ–Ω–¥–æ–≤
                    for child in token.children:
                        if child.lemma_ in brand_indicators:
                            result['brands'].append(token.text)
                
                # –ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–µ–π (–æ–±—ã—á–Ω–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–∏ –±—É–∫–≤ –∏ —Ü–∏—Ñ—Ä)
                if any(char.isdigit() for char in token.text) and any(char.isalpha() for char in token.text):
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π
                    for child in token.children:
                        if child.lemma_ in model_indicators:
                            result['models'].append(token.text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        result['brands'] = list(set(result['brands']))
        result['models'] = list(set(result['models']))
        
        return result
    
    def extract_technical_terms(self, text: str) -> List[str]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            List: –°–ø–∏—Å–æ–∫ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤
        """
        technical_terms = []
        
        if not text:
            return technical_terms
        
        doc = self.nlp(text)
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
        tech_patterns = [
            r'\d+[.,]?\d*\s*(–º–º|—Å–º|–º|–∫–≥|–≥|–ª|–º–ª|–ì—Ü|–∫–ì—Ü|–ú–ì—Ü|–í—Ç|–∫–í—Ç|–í|–ê|—á|¬∞C)',
            r'\b(–¥–∏–∞–º–µ—Ç—Ä|–¥–ª–∏–Ω–∞|—à–∏—Ä–∏–Ω–∞|–≤—ã—Å–æ—Ç–∞|–≥–ª—É–±–∏–Ω–∞|–≤–µ—Å|–æ–±—ä–µ–º|–º–æ—â–Ω–æ—Å—Ç—å|–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ|—Ç–æ–∫|—á–∞—Å—Ç–æ—Ç–∞)\b',
            r'\b(–º–∞—Ç–µ—Ä–∏–∞–ª|—Ü–≤–µ—Ç|—Ä–∞–∑–º–µ—Ä|–≥–∞–±–∞—Ä–∏—Ç—ã|—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ?|—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏?|–ø–∞—Ä–∞–º–µ—Ç—Ä—ã?)\b'
        ]
        
        import re
        for pattern in tech_patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE)
            for match in matches:
                technical_terms.append(match.group())
        
        return list(set(technical_terms))
    
    def restructure_sentence(self, sentence: str, preserved_terms: Dict[str, List[str]]) -> str:
        """
        –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        
        Args:
            sentence: –ò—Å—Ö–æ–¥–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
            preserved_terms: –¢–µ—Ä–º–∏–Ω—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            
        Returns:
            str: –ü–µ—Ä–µ—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        """
        doc = self.nlp(sentence)
        
        # –ü—Ä–æ—Å—Ç–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã - –∏–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞ —Å–ª–æ–≤
        words = [token.text for token in doc if not token.is_punct]
        
        if len(words) <= 3:
            return sentence  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
        
        # –†–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–µ—Ä–µ—Å—Ç—Ä–æ–π–∫–∏
        import random
        strategies = [
            self._reverse_clauses,
            self._change_voice,
            self._reorder_phrases
        ]
        
        strategy = random.choice(strategies)
        return strategy(sentence, preserved_terms)
    
    def _reverse_clauses(self, sentence: str, preserved_terms: Dict[str, List[str]]) -> str:
        """–ò–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞ —á–∞—Å—Ç–µ–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        doc = self.nlp(sentence)
        
        # –ü–æ–∏—Å–∫ —Å–æ—é–∑–æ–≤ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è –Ω–∞ —á–∞—Å—Ç–∏
        clauses = []
        current_clause = []
        
        for token in doc:
            current_clause.append(token.text)
            if token.dep_ == 'cc' or token.text in [',', ';']:
                clauses.append(' '.join(current_clause))
                current_clause = []
        
        if current_clause:
            clauses.append(' '.join(current_clause))
        
        if len(clauses) > 1:
            # –ú–µ–Ω—è–µ–º –ø–æ—Ä—è–¥–æ–∫ —á–∞—Å—Ç–µ–π
            import random
            random.shuffle(clauses)
            return ' '.join(clauses)
        else:
            return sentence
    
    def _change_voice(self, sentence: str, preserved_terms: Dict[str, List[str]]) -> str:
        """–ò–∑–º–µ–Ω–µ–Ω–∏–µ –∑–∞–ª–æ–≥–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–∞–∫—Ç–∏–≤–Ω—ã–π/–ø–∞—Å—Å–∏–≤–Ω—ã–π)"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–µ —ç—Ç–æ —Ç—Ä–µ–±–æ–≤–∞–ª–æ –±—ã —Å–ª–æ–∂–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        return sentence  # –ó–∞–≥–ª—É—à–∫–∞
    
    def _reorder_phrases(self, sentence: str, preserved_terms: Dict[str, List[str]]) -> str:
        """–ò–∑–º–µ–Ω–µ–Ω–∏–µ –ø–æ—Ä—è–¥–∫–∞ —Ñ—Ä–∞–∑ –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏"""
        words = sentence.split()
        
        if len(words) > 4:
            # –ú–µ–Ω—è–µ–º –º–µ—Å—Ç–∞–º–∏ –ø–µ—Ä–≤—ã–µ –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–ª–æ–≤–∞
            words[0], words[-1] = words[-1], words[0]
            return ' '.join(words)
        else:
            return sentence
    
    def paraphrase_sentence(self, sentence: str, preserved_terms: Dict[str, List[str]]) -> str:
        """
        –ü–∞—Ä–∞—Ñ—Ä–∞–∑ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        
        Args:
            sentence: –ò—Å—Ö–æ–¥–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
            preserved_terms: –¢–µ—Ä–º–∏–Ω—ã –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            
        Returns:
            str: –ü–∞—Ä–∞—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–º–±–∏–Ω–∞—Ü–∏—é –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è –ø–∞—Ä–∞—Ñ—Ä–∞–∑–∞
        methods = [
            self._replace_with_synonyms_simple,
            self._simplify_sentence,
            self._expand_sentence
        ]
        
        import random
        method = random.choice(methods)
        return method(sentence, preserved_terms)
    
    def _replace_with_synonyms_simple(self, sentence: str, preserved_terms: Dict[str, List[str]]) -> str:
        """–ü—Ä–æ—Å—Ç–∞—è –∑–∞–º–µ–Ω–∞ —Å–ª–æ–≤ —Å–∏–Ω–æ–Ω–∏–º–∞–º–∏"""
        # –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –±—É–¥–µ—Ç –¥–æ–ø–æ–ª–Ω–µ–Ω–∞ –≤ synonym_manager
        return sentence
    
    def _simplify_sentence(self, sentence: str, preserved_terms: Dict[str, List[str]]) -> str:
        """–£–ø—Ä–æ—â–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è"""
        doc = self.nlp(sentence)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –∏ –Ω–∞—Ä–µ—á–∏–π
        simplified_words = []
        for token in doc:
            if token.pos_ in ['ADJ', 'ADV']:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ/–Ω–∞—Ä–µ—á–∏—è —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é
                import random
                if random.random() < 0.3:
                    continue
            
            simplified_words.append(token.text)
        
        return ' '.join(simplified_words)
    
    def _expand_sentence(self, sentence: str, preserved_terms: Dict[str, List[str]]) -> str:
        """–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏"""
        expanders = [
            '–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π', '–Ω–∞–¥–µ–∂–Ω—ã–π', '–ø–æ–ø—É–ª—è—Ä–Ω—ã–π', '—Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π',
            '–ø—Ä–∞–∫—Ç–∏—á–Ω—ã–π', '—Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π', '—Å—Ç–∏–ª—å–Ω—ã–π'
        ]
        
        import random
        if random.random() < 0.5:
            expanded = f"{random.choice(expanders)} {sentence}"
            return expanded
        else:
            return sentence
    
    def correct_grammar(self, text: str) -> str:
        """
        –ö–æ—Ä—Ä–µ–∫—Ü–∏—è –≥—Ä–∞–º–º–∞—Ç–∏–∫–∏ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ü–∏–∏
            
        Returns:
            str: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        # –ë–∞–∑–æ–≤–∞—è –∫–æ—Ä—Ä–µ–∫—Ü–∏—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        import re
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–µ–ª–æ–≤ –ø–æ—Å–ª–µ –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è
        text = re.sub(r'([.!?])([–ê-–Ø–∞-—è])', r'\1 \2', text)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        text = re.sub(r' +', ' ', text)
        
        # –ö–æ—Ä—Ä–µ–∫—Ü–∏—è —Ä–µ–≥–∏—Å—Ç—Ä–∞ –≤ –Ω–∞—á–∞–ª–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'([.!?] )', text)
        corrected_sentences = []
        
        for i, sentence in enumerate(sentences):
            if i % 2 == 0 and sentence:  # –û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
                if sentence and sentence[0].islower():
                    sentence = sentence[0].upper() + sentence[1:]
                corrected_sentences.append(sentence)
            else:  # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
                corrected_sentences.append(sentence)
        
        return ''.join(corrected_sentences)
    
    def get_word_lemma(self, word: str) -> str:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –ª–µ–º–º—ã —Å–ª–æ–≤–∞
        
        Args:
            word: –°–ª–æ–≤–æ –¥–ª—è –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏
            
        Returns:
            str: –õ–µ–º–º–∞ —Å–ª–æ–≤–∞
        """
        try:
            parsed = self.morph_analyzer.parse(word)[0]
            return parsed.normal_form
        except Exception:
            return word
    
    def get_word_morphology(self, word: str) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–ª–æ–≤–µ
        
        Args:
            word: –°–ª–æ–≤–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            Dict: –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        """
        try:
            parsed = self.morph_analyzer.parse(word)[0]
            return {
                'normal_form': parsed.normal_form,
                'tag': str(parsed.tag),
                'score': parsed.score,
                'methods': parsed.methods_stack
            }
        except Exception as e:
            return {
                'normal_form': word,
                'tag': 'UNKN',
                'error': str(e)
            }
