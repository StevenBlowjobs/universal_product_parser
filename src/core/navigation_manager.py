#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–∞–≤–∏–≥–∞—Ü–∏–µ–π –ø–æ —Å–∞–π—Ç—É –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
"""

import asyncio
import re
from typing import Dict, List, Optional, Any, Tuple
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
from playwright.async_api import Page
from ..utils.logger import setup_logger
from ..utils.error_handler import retry_on_failure


class NavigationManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º –∏ –ø–∞–≥–∏–Ω–∞—Ü–∏–∏"""
    
    def __init__(self, page: Page, detected_selectors: Dict[str, str]):
        self.page = page
        self.selectors = detected_selectors
        self.logger = setup_logger("navigation_manager")
        self.base_url = None
        
    async def get_categories(self) -> List[Dict[str, str]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ç–æ–≤–∞—Ä–æ–≤"""
        self.logger.info("üìÇ –ü–æ–∏—Å–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ç–æ–≤–∞—Ä–æ–≤...")
        
        categories = []
        
        try:
            # –ü–æ–ª—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ URL
            self.base_url = await self._get_base_url()
            
            # –ü–æ–∏—Å–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏
            nav_categories = await self._find_navigation_categories()
            categories.extend(nav_categories)
            
            # –ü–æ–∏—Å–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –∫–∞—Ä—Ç–µ —Å–∞–π—Ç–∞
            sitemap_categories = await self._find_sitemap_categories()
            categories.extend(sitemap_categories)
            
            # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
            unique_categories = self._remove_duplicate_categories(categories)
            
            self.logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(unique_categories)}")
            return unique_categories
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {e}")
            return []
    
    async def _get_base_url(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ URL —Å–∞–π—Ç–∞"""
        current_url = self.page.url
        parsed = urlparse(current_url)
        return f"{parsed.scheme}://{parsed.netloc}"
    
    async def _find_navigation_categories(self) -> List[Dict[str, str]]:
        """–ü–æ–∏—Å–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ–Ω—é"""
        categories = []
        
        # –†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –º–µ–Ω—é
        nav_selectors = [
            'nav ul li a',
            '.navigation a',
            '.menu a',
            '.categories a',
            '.nav-menu a',
            '[class*="nav"] a',
            '[class*="menu"] a',
            '[class*="category"] a'
        ]
        
        for selector in nav_selectors:
            try:
                elements = await self.page.query_selector_all(selector)
                for element in elements:
                    category = await self._extract_category_from_element(element)
                    if category and self._is_valid_category(category):
                        categories.append(category)
            except Exception as e:
                self.logger.debug(f"–°–µ–ª–µ–∫—Ç–æ—Ä {selector} –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {e}")
                continue
        
        return categories
    
    async def _extract_category_from_element(self, element) -> Optional[Dict[str, str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        try:
            name = await element.text_content()
            href = await element.get_attribute('href')
            
            if not name or not href:
                return None
            
            name = name.strip()
            if len(name) > 100:  # –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ - –≤–µ—Ä–æ—è—Ç–Ω–æ –Ω–µ –∫–∞—Ç–µ–≥–æ—Ä–∏—è
                return None
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ URL
            full_url = urljoin(self.base_url, href)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–æ–≤
            if await self._is_product_category(full_url):
                return {
                    'name': name,
                    'url': full_url,
                    'type': 'navigation'
                }
                
        except Exception as e:
            self.logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {e}")
            
        return None
    
    async def _is_product_category(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ URL –≤–µ–¥–µ—Ç –Ω–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ç–æ–≤–∞—Ä–æ–≤"""
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –≤ URL
            category_patterns = [
                r'/categor', r'/category', r'/kategor', r'/kategoria',
                r'/shop', r'/products', r'/tovary', r'/catalog',
                r'/collection', r'/brand', r'/type'
            ]
            
            if any(re.search(pattern, url, re.IGNORECASE) for pattern in category_patterns):
                return True
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–º—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            await self.page.goto(url, wait_until='domcontentloaded')
            content = await self.page.content()
            
            # –ü–æ–∏—Å–∫ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ —Ç–æ–≤–∞—Ä–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            indicators = [
                'product', '—Ç–æ–≤–∞—Ä', 'price', '—Ü–µ–Ω–∞', 'buy', '–∫—É–ø–∏—Ç—å',
                'add to cart', '–≤ –∫–æ—Ä–∑–∏–Ω—É', '—Ç–æ–≤–∞—Ä–æ–≤', 'products'
            ]
            
            soup = BeautifulSoup(content, 'html.parser')
            text_content = soup.get_text().lower()
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ - –≤–µ—Ä–æ—è—Ç–Ω–æ —ç—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è —Ç–æ–≤–∞—Ä–æ–≤
            indicator_count = sum(1 for indicator in indicators if indicator in text_content)
            return indicator_count >= 2
            
        except Exception as e:
            self.logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {url}: {e}")
            return False
    
    async def _find_sitemap_categories(self) -> List[Dict[str, str]]:
        """–ü–æ–∏—Å–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —á–µ—Ä–µ–∑ sitemap.xml"""
        categories = []
        
        sitemap_urls = [
            f"{self.base_url}/sitemap.xml",
            f"{self.base_url}/sitemap_index.xml",
            f"{self.base_url}/sitemap/categories.xml",
            f"{self.base_url}/sitemap-products.xml"
        ]
        
        for sitemap_url in sitemap_urls:
            try:
                await self.page.goto(sitemap_url, wait_until='networkidle')
                content = await self.page.content()
                
                if '<?xml' in content:
                    # –ü–∞—Ä—Å–∏–Ω–≥ XML sitemap
                    soup = BeautifulSoup(content, 'xml')
                    urls = soup.find_all('loc')
                    
                    for url in urls[:50]:  # –û–≥—Ä–∞–Ω–∏—á–∏–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                        url_text = url.get_text()
                        if self._is_category_url(url_text):
                            categories.append({
                                'name': self._extract_category_name_from_url(url_text),
                                'url': url_text,
                                'type': 'sitemap'
                            })
                            
            except Exception as e:
                self.logger.debug(f"Sitemap {sitemap_url} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                continue
        
        return categories
    
    def _is_category_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ URL —è–≤–ª—è–µ—Ç—Å—è –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π"""
        category_indicators = [
            '/category/', '/categor/', '/kategor/', '/shop/',
            '/products/', '/catalog/', '/collection/'
        ]
        return any(indicator in url.lower() for indicator in category_indicators)
    
    def _extract_category_name_from_url(self, url: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ URL"""
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π —á–∞—Å—Ç–∏ URL –∫–∞–∫ –Ω–∞–∑–≤–∞–Ω–∏—è
        name = url.rstrip('/').split('/')[-1]
        # –ó–∞–º–µ–Ω–∞ –¥–µ—Ñ–∏—Å–æ–≤ –∏ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–π –Ω–∞ –ø—Ä–æ–±–µ–ª—ã
        name = re.sub(r'[-_]', ' ', name)
        # Capitalize
        return name.title()
    
    def _is_valid_category(self, category: Dict[str, str]) -> bool:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"""
        name = category.get('name', '')
        url = category.get('url', '')
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª–∏–Ω—ã –Ω–∞–∑–≤–∞–Ω–∏—è
        if len(name) < 2 or len(name) > 100:
            return False
        
        # –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –æ–±—â–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        exclude_keywords = ['home', 'main', 'index', 'contact', 'about', 'login', 'register']
        if any(keyword in name.lower() for keyword in exclude_keywords):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ URL
        if not url.startswith(('http://', 'https://')):
            return False
        
        return True
    
    def _remove_duplicate_categories(self, categories: List[Dict]) -> List[Dict]:
        """–£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–π"""
        seen_urls = set()
        unique_categories = []
        
        for category in categories:
            url = category['url']
            if url not in seen_urls:
                seen_urls.add(url)
                unique_categories.append(category)
        
        return unique_categories
    
    @retry_on_failure(max_retries=2)
    async def parse_category(self, category_url: str, filters: Optional[Dict] = None) -> List[Dict]:
        """
        –ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Å —É—á–µ—Ç–æ–º —Ñ–∏–ª—å—Ç—Ä–æ–≤
        
        Args:
            category_url: URL –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            filters: –§–∏–ª—å—Ç—Ä—ã —Ç–æ–≤–∞—Ä–æ–≤ (—Ü–µ–Ω–∞, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –∏ —Ç.–¥.)
            
        Returns:
            List[Dict]: –°–ø–∏—Å–æ–∫ —Ç–æ–≤–∞—Ä–æ–≤ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        """
        self.logger.info(f"üì¶ –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {category_url}")
        
        try:
            await self.page.goto(category_url, wait_until='networkidle')
            
            products = []
            page_number = 1
            
            while True:
                self.logger.info(f"üìÑ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_number}")
                
                # –ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                page_products = await self._parse_products_on_page(filters)
                products.extend(page_products)
                
                self.logger.info(f"‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_number}: {len(page_products)} —Ç–æ–≤–∞—Ä–æ–≤")
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –ø–µ—Ä–µ—Ö–æ–¥–∞ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                next_page_url = await self._get_next_page_url()
                if not next_page_url:
                    break
                
                # –ü–µ—Ä–µ—Ö–æ–¥ –Ω–∞ —Å–ª–µ–¥—É—é—â—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                await self.page.goto(next_page_url, wait_until='networkidle')
                page_number += 1
                
                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                if page_number > 10:  # –ú–∞–∫—Å–∏–º—É–º 10 —Å—Ç—Ä–∞–Ω–∏—Ü
                    self.logger.warning("‚ö†Ô∏è  –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç —Å—Ç—Ä–∞–Ω–∏—Ü (10)")
                    break
            
            self.logger.info(f"üéâ –ö–∞—Ç–µ–≥–æ—Ä–∏—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞: {len(products)} —Ç–æ–≤–∞—Ä–æ–≤")
            return products
            
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {e}")
            return []
    
    async def _parse_products_on_page(self, filters: Optional[Dict]) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
        products = []
        
        # –ü–æ–∏—Å–∫ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Ç–æ–≤–∞—Ä–æ–≤
        product_selectors = [
            '.product', '.item', '.goods', '.product-item',
            '[class*="product"]', '[class*="item"]', '.card'
        ]
        
        for selector in product_selectors:
            try:
                product_elements = await self.page.query_selector_all(selector)
                if product_elements:
                    self.logger.debug(f"–ù–∞–π–¥–µ–Ω—ã —Ç–æ–≤–∞—Ä—ã –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä—É: {selector}")
                    break
            except Exception:
                continue
        
        if not product_elements:
            self.logger.warning("‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω—ã —ç–ª–µ–º–µ–Ω—Ç—ã —Ç–æ–≤–∞—Ä–æ–≤")
            return []
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞
        for element in product_elements:
            try:
                product_data = await self._extract_product_from_element(element)
                if product_data and self._apply_filters(product_data, filters):
                    products.append(product_data)
            except Exception as e:
                self.logger.debug(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ç–æ–≤–∞—Ä–∞: {e}")
                continue
        
        return products
    
    async def _extract_product_from_element(self, element) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–∞ –∏–∑ —ç–ª–µ–º–µ–Ω—Ç–∞"""
        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∞–∑–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
            name_element = await element.query_selector('h3, h4, .title, .name, [class*="title"]')
            name = await name_element.text_content() if name_element else None
            
            price_element = await element.query_selector('.price, .cost, [class*="price"]')
            price_text = await price_element.text_content() if price_element else None
            
            link_element = await element.query_selector('a')
            product_url = await link_element.get_attribute('href') if link_element else None
            
            if not name or not product_url:
                return None
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ URL —Ç–æ–≤–∞—Ä–∞
            full_url = urljoin(self.base_url, product_url)
            
            product_data = {
                'name': name.strip(),
                'url': full_url,
                'price': self._extract_price_from_text(price_text) if price_text else None,
                'image_url': await self._extract_image_url(element),
                'category_page_data': True  # –§–ª–∞–≥, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —Å —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            }
            
            return product_data
            
        except Exception as e:
            self.logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–∞: {e}")
            return None
    
    def _extract_price_from_text(self, price_text: str) -> Optional[float]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        try:
            # –û—á–∏—Å—Ç–∫–∞ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ü–µ–Ω—ã
            price_clean = re.sub(r'[^\d,.]', '', price_text)
            price_clean = price_clean.replace(',', '.')
            price_match = re.search(r'\d+\.?\d*', price_clean)
            return float(price_match.group()) if price_match else None
        except (ValueError, TypeError):
            return None
    
    async def _extract_image_url(self, element) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Ç–æ–≤–∞—Ä–∞"""
        try:
            img_element = await element.query_selector('img')
            if img_element:
                src = await img_element.get_attribute('src')
                return urljoin(self.base_url, src) if src else None
        except Exception:
            pass
        return None
    
    def _apply_filters(self, product_data: Dict, filters: Optional[Dict]) -> bool:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ –∫ —Ç–æ–≤–∞—Ä—É"""
        if not filters:
            return True
        
        # –§–∏–ª—å—Ç—Ä –ø–æ —Ü–µ–Ω–µ
        if 'price_range' in filters:
            price_range = filters['price_range']
            product_price = product_data.get('price')
            
            if product_price is not None:
                min_price = price_range.get('min', 0)
                max_price = price_range.get('max', float('inf'))
                
                if not (min_price <= product_price <= max_price):
                    return False
        
        # –§–∏–ª—å—Ç—Ä –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        if 'categories' in filters and filters['categories']:
            category = product_data.get('category', '')
            if not any(cat.lower() in category.lower() for cat in filters['categories']):
                return False
        
        return True
    
    async def _get_next_page_url(self) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ URL —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–∞–≥–∏–Ω–∞—Ü–∏–∏"""
        try:
            # –ü–æ–∏—Å–∫ –∫–Ω–æ–ø–∫–∏ "–°–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"
            next_selectors = [
                '.pagination .next a',
                '.pagination a[rel="next"]',
                '.next-page',
                '.load-more',
                'a:has-text("–î–∞–ª–µ–µ")',
                'a:has-text("Next")',
                'button:has-text("–ï—â–µ —Ç–æ–≤–∞—Ä—ã")'
            ]
            
            for selector in next_selectors:
                next_element = await self.page.query_selector(selector)
                if next_element:
                    href = await next_element.get_attribute('href')
                    if href:
                        return urljoin(self.base_url, href)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–º–µ—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü
            page_links = await self.page.query_selector_all('.pagination a, .page-numbers a')
            current_url = self.page.url
            
            for link in page_links:
                href = await link.get_attribute('href')
                link_text = await link.text_content()
                
                if href and href != current_url and link_text.strip().isdigit():
                    page_num = int(link_text.strip())
                    if page_num > self._get_current_page_number(current_url):
                        return urljoin(self.base_url, href)
                        
        except Exception as e:
            self.logger.debug(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        
        return None
    
    def _get_current_page_number(self, url: str) -> int:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–æ–º–µ—Ä–∞ —Ç–µ–∫—É—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–∑ URL"""
        page_match = re.search(r'page[=/]?(\d+)', url, re.IGNORECASE)
        return int(page_match.group(1)) if page_match else 1
